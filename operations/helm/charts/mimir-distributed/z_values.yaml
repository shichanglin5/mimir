# Note:
# 1. scrapeK8s kube-state-metrics 需要配置 namespace 和 labelSelectors
# 2. metaMonitoring.grafanaAgent.remote.url 自动生成 distributor /api/v1/push 地址
# 3. 创建namespace 和 extraEnv CM: kubectl create namespace mimir && kubectl create configmap mimir-extra-env -n mimir
# 4. mimir-config中 no_auth_tenant 会作为默认的 X-Scope-OrgID 请求头，用于 lens 访问 metrics，配置为 mimir（mimir相关指标写入用的租户也是 mimir）

kubeVersionOverride: v1.27.4
fullnameOverride: mimir
image:
  repository: hub.17usoft.com/lhhdz/grafana/mimir
  tag: 2.11_pre
  pullPolicy: Always
global:
  dnsService: kube-dns
  dnsNamespace: kube-system
  clusterDomain: cluster.local.
  extraEnv: [ ]
  extraEnvFrom:
    - configMapRef:
        name: mimir-extra-env
serviceAccount:
  create: true
configStorageType: ConfigMap
runtimeConfig:
  ingester_limits:
    max_inflight_push_requests: 200000000
    max_series: 200000000
    max_tenants: 50
    max_ingestion_rate: 200000000
  distributor_limits:
    max_ingestion_rate: 200000000
    max_inflight_push_requests: 200000000
    max_inflight_push_requests_bytes: 10000000000
  overrides:
    anonymous:
      request_rate: 100
      request_burst_size: 200
      ingestion_rate: 3000000
      ingestion_burst_size: 6000000
      max_global_series_per_user: 200000000
      max_global_series_per_metric: 200000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    mimir:
      request_rate: 1000
      request_burst_size: 2000
      ingestion_rate: 3000000
      ingestion_burst_size: 6000000
      max_global_series_per_user: 200000000
      max_global_series_per_metric: 200000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
      max_label_name_length: 200
      max_label_value_length: 200
    default:
      request_rate: 5000
      request_burst_size: 5000
      ingestion_rate: 2000000
      ingestion_burst_size: 2000000
      max_global_series_per_user: 15000000
      max_global_series_per_metric: 15000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-tlb:
      request_rate: 10000
      request_burst_size: 14000
      ingestion_rate: 500000
      ingestion_burst_size: 800000
      max_global_series_per_user: 15000000
      max_global_series_per_metric: 15000000
      out_of_order_time_window: 10m
      max_cache_freshness: 20m
    skyeye-tlb-uat:
      request_rate: 1000
      request_burst_size: 1500
      ingestion_rate: 50000
      ingestion_burst_size: 500000
      max_global_series_per_user: 500000
      max_global_series_per_metric: 500000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-skynet:
      request_rate: 1000
      request_burst_size: 1500
      ingestion_rate: 90000
      ingestion_burst_size: 100000
      max_global_series_per_user: 400000
      max_global_series_per_metric: 400000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-apm:
      request_rate: 60000
      request_burst_size: 70000
      ingestion_rate: 3000000
      ingestion_burst_size: 4000000
      max_global_series_per_user: 120000000
      max_global_series_per_metric: 120000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-apm-uat:
      request_rate: 10000
      request_burst_size: 15000
      ingestion_rate: 500000
      ingestion_burst_size: 600000
      max_global_series_per_user: 3000000
      max_global_series_per_metric: 3000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    inf-redisproxy:
      request_rate: 1000
      request_burst_size: 1000
      ingestion_rate: 500000
      ingestion_burst_size: 600000
      max_global_series_per_user: 2000000
      max_global_series_per_metric: 2000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    inf-dbproxy:
      request_rate: 2000
      request_burst_size: 3000
      ingestion_rate: 2000000
      ingestion_burst_size: 2500000
      max_global_series_per_user: 20000000
      max_global_series_per_metric: 20000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    inf-esproxy:
      request_rate: 4000
      request_burst_size: 5000
      ingestion_rate: 4000000
      ingestion_burst_size: 5000000
      max_global_series_per_user: 15000000
      max_global_series_per_metric: 15000000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    inf-es:
      request_rate: 1
      ingestion_rate: 1
      ingestion_burst_size: 1
      max_global_series_per_user: 1
      max_global_series_per_metric: 1
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-gateway:
      request_rate: 5000
      request_burst_size: 5000
      ingestion_rate: 400000
      ingestion_burst_size: 400000
      max_global_series_per_user: 1800000
      max_global_series_per_metric: 1800000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m
    skyeye-alert:
      request_rate: 100
      request_burst_size: 200
      ingestion_rate: 500
      ingestion_burst_size: 100
      max_global_series_per_user: 10000
      max_global_series_per_metric: 10000
      out_of_order_time_window: 10m
      max_cache_freshness: 12m

distributor:
  replicas: 8
  nodeSelector:
    schedule.enabled/mimir: ''
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
  terminationGracePeriodSeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 15%
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

ingester:
  replicas: 8
  nodeSelector:
    schedule.enabled/mimir: ''
  statefulSet:
    enabled: true
  persistentVolume:
    enabled: true
    storageClass: "mimir-ingester"
    size: 800Gi
    accessModes:
      - ReadWriteOnce
    enableRetentionPolicy: true
    whenScaled: Retain
    whenDeleted: Retain
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
    initialDelaySeconds: 60
    timeoutSeconds: 3
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
  zoneAwareReplication:
    enabled: false
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

overrides_exporter:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

ruler:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 0
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

querier:
  replicas: 8
  nodeSelector:
    schedule.enabled/mimir: ''
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

query_frontend:
  replicas: 8
  nodeSelector:
    schedule.enabled/mimir: ''
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

query_scheduler:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 2
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

store_gateway:
  replicas: 0
  nodeSelector:
    schedule.enabled/mimir: ''
  podDisruptionBudget:
    maxUnavailable: 1
  persistentVolume:
    enabled: true
    storageClass: "mimir-store-gateway"
    size: 600Gi
    accessModes:
      - ReadWriteOnce
    enableRetentionPolicy: true
    whenScaled: Retain
    whenDeleted: Retain
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 60
  strategy:
    type: RollingUpdate
  zoneAwareReplication:
    enabled: false
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

compactor:
  replicas: 0
  nodeSelector:
    schedule.enabled/mimir: ''
  persistentVolume:
    enabled: true
    storageClass: "mimir-compactor"
    size: 300Gi
    accessModes:
      - ReadWriteOnce
    enableRetentionPolicy: true
    whenScaled: Retain
    whenDeleted: Retain
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  podSecurityContext: ~
  containerSecurityContext: ~
  resources:
    limits:
      memory: ~
    requests:
      cpu: ~
      memory: ~

chunks-cache:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  resources:
    requests:
      cpu: ~
      memory: ~

index-cache:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  resources:
    requests:
      cpu: ~
      memory: ~

metadata-cache:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  resources:
    requests:
      cpu: ~
      memory: ~

results-cache:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  allocatedMemory: 1024
  resources:
    requests:
      cpu: ~
      memory: ~

metaMonitoring:
  dashboards:
    enabled: true
# 基于 dashboard 目录结构生成
#    annotations:
#      k8s-sidecar-target-directory: 'Mimir Dashboards' # 如果这里用绝对路径，会导致 sidecar 将文件保存到绝对路径，会导致grafana读取不到dashboard；如果相对路径，会拼接到 /tmp/dashboards/ 下
    labels:
      grafana_dashboard: "1"
  serviceMonitor:
    enabled: true
    clusterLabel: taihu
    interval: 5s
    scrapeTimeout: 3s
    scheme: http
  grafanaAgent:
    enabled: true
    imageRepo:
    #  configReloader:
    #    repo: quay.io
    #    image: prometheus-operator/prometheus-config-reloader
    #    tag: v0.47.0
    #  grafanaAgent:
    #    repo: docker.io
    #    image: grafana/agent
    #    tag: v0.29.0
    installOperator: true
    logs:
      enabled: false
    metrics:
      enabled: true
      remote:
        #url 自动生成 distributor /api/v1/push 地址
        headers:
          X-Scope-OrgID: mimir
      scrapeK8s:
        enabled: true
        kubeStateMetrics:
          namespace: lens-metrics
          labelSelectors:
            name: kube-state-metrics
            app.kubernetes.io/name: ~
      scrapeInterval: 15s
    securityContext:
      fsGroup: ~
      runAsGroup: ~
      runAsNonRoot: ~
      runAsUser: 0
      seccompProfile: ~
    podSecurityContext: ~
    containerSecurityContext: ~

# -- Grafana Agent Operator 覆盖默认配置
# 在一些 k8s 集群配置，可能会禁用一些默认的配置，比如 PodSecurityPolicy，这里可以覆盖默认配置
# 导致 runtime/cgo: pthread_create failed: Operation not permitted
grafana-agent-operator:
  containerSecurityContext: ~
  podSecurityContext: ~
  nodeSelector:
    schedule.enabled/mimir: ''

nginx:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  replicas: 1
  securityContext:
    fsGroup: ~
    runAsGroup: ~
    runAsNonRoot: ~
    runAsUser: 0
    seccompProfile: ~
  verboseLogging: true
  image:
    registry: docker.io
    repository: nginxinc/nginx-unprivileged
    tag: 1.25-alpine
    pullPolicy: IfNotPresent
  readinessProbe: ~
#    httpGet:
#      path: /
#      port: http-metric
#    initialDelaySeconds: 5
#    timeoutSeconds: 1
  nginxConfig:
    logFormat: |-
      main '$remote_addr - $remote_user [$time_local]  $status '
              '"$request" $body_bytes_sent "$http_referer" '
              '"$http_user_agent" "$http_x_forwarded_for"';
    # -- Sets the log level of the NGINX error log. One of `debug`, `info`, `notice`, `warn`, `error`, `crit`, `alert`, or `emerg`
    errorLogLevel: error
    accessLogEnabled: true
    file: |
      worker_processes  5;  ## Default: 1
      error_log  /dev/stderr {{ .Values.nginx.nginxConfig.errorLogLevel }};
      pid        /tmp/nginx.pid;
      worker_rlimit_nofile 8192;

      events {
        worker_connections  4096;  ## Default: 1024
      }

      http {
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path       /tmp/proxy_temp_path;
        fastcgi_temp_path     /tmp/fastcgi_temp;
        uwsgi_temp_path       /tmp/uwsgi_temp;
        scgi_temp_path        /tmp/scgi_temp;

        default_type application/octet-stream;
        log_format   {{ .Values.nginx.nginxConfig.logFormat }}

        {{- if .Values.nginx.verboseLogging }}
        access_log   /dev/stderr  main;
        {{- else }}

        map $status $loggable {
          ~^[23]  0;
          default 1;
        }
        access_log   {{ .Values.nginx.nginxConfig.accessLogEnabled | ternary "/dev/stderr  main  if=$loggable;" "off;" }}
        {{- end }}

        sendfile     on;
        tcp_nopush   on;

        {{- if .Values.nginx.nginxConfig.resolver }}
        resolver {{ .Values.nginx.nginxConfig.resolver }};
        {{- else }}
        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
        {{- end }}

        {{- with .Values.nginx.nginxConfig.httpSnippet }}
        {{ . | nindent 2 }}
        {{- end }}

        # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
        map $http_x_scope_orgid $ensured_x_scope_orgid {
          default $http_x_scope_orgid;
          "" "{{ include "mimir.noAuthTenant" . }}";
        }

        proxy_read_timeout 300;
        server {
          listen 8080;
          listen [::]:8080;

          {{- if .Values.nginx.basicAuth.enabled }}
          auth_basic           "Mimir";
          auth_basic_user_file /etc/nginx/secrets/.htpasswd;
          {{- end }}

          location = / {
            return 200 'OK';
            auth_basic off;
          }

          proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

          # Distributor endpoints
          location /distributor {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /api/v1/push {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location /otlp/v1/metrics {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Alertmanager endpoints
          location {{ template "mimir.alertmanagerHttpPrefix" . }} {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /multitenant_alertmanager/status {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /api/v1/alerts {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Ruler endpoints
          location {{ template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /ruler/ring {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Rest of {{ template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
          location {{ template "mimir.prometheusHttpPrefix" . }} {
            set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Buildinfo endpoint can go to any component
          location = /api/v1/status/buildinfo {
            set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Compactor endpoint for uploading blocks
          location /api/v1/upload/block/ {
            set $compactor {{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$compactor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          {{- with .Values.nginx.nginxConfig.serverSnippet }}
          {{ . | nindent 4 }}
          {{- end }}
        }
      }

grafana:
  enabled: true
  nodeSelector:
    schedule.enabled/mimir: ''
  image:
    registry:  hub.17usoft.com
    repository: lhhdz/grafana/grafana
    tag: 10.2.3
    pullPolicy: IfNotPresent
  testFramework:
    enabled: false
  adminUser: admin
  adminPassword: admin
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Mimir
        type: prometheus
        url: http://mimir-nginx.mimir.svc.cluster.local/prometheus
        access: proxy
        isDefault: true
  sidecar:
    image:
      registry: hub.17usoft.com
      repository: lhhdz/kiwigrid/k8s-sidecar
      tag: 1.25.2
    dashboards:
      enabled: true
      provider:
        name: 'sidecarProvider'
        orgId: 1
        type: file
        disableDeletion: false
        allowUiUpdates: false
        updateIntervalSeconds: 15
        foldersFromFilesStructure: true
        options:
          path: /tmp/dashboards
      label: grafana_dashboard
      labelValue: "1"
      searchNamespace: mimir
      folder: /tmp/dashboards # pod 中的目录
      folderAnnotation: k8s-sidecar-target-directory # sidecar 会根据这个 annotation 来决定 dashboard 的目录
#  dashboardProviders:
#    sidecarProvider:
#      apiVersion: 1
#      providers:
#      - name: mimir
#        orgId: 1
#        folder: 'mimir'
#        type: file
#        disableDeletion: false
#        allowUiUpdates: false
#        updateIntervalSeconds: 30
#        options:
#          foldersFromFilesStructure: false
#          path: /var/lib/grafana/dashboards/mimir
#      - name: kubernetes
#        orgId: 1
#        folder: 'kubernetes'
#        type: file
#        disableDeletion: false
#        allowUiUpdates: false
#        updateIntervalSeconds: 30
#        options:
#          foldersFromFilesStructure: false
#          path: /var/lib/grafana/dashboards/kubernetes
#      - name: node-exporter
#        orgId: 1
#        folder: 'node-exporter'
#        type: file
#        disableDeletion: false
#        allowUiUpdates: false
#        updateIntervalSeconds: 30
#        options:
#          foldersFromFilesStructure: false
#          path: /var/lib/grafana/dashboards/node-exporter
  securityContext:
    runAsUser: 0
    runAsGroup: 0
    runAsNonRoot: false
    fsGroup: 0

rollout_operator:
  enabled: false
  nodeSelector:
    schedule.enabled/mimir: ''
  podSecurityContext: ~
  securityContext: ~
enterprise:
  enabled: false
admin-cache:
  enabled: false
alertmanager:
  enabled: false
minio:
  enabled: false